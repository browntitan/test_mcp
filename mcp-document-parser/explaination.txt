Our original session-duration calculation used the chat table as an “event stream” because the messages table is empty in our deployment. In our database, the chat table is essentially one row per chat thread, with created_at and updated_at stored as epoch timestamps (BIGINT), and a chat JSON blob that contains the conversation content/metadata. We don’t currently have a populated, normalized message table that stores one row per message with its own timestamp, so we can’t reliably compute “time in session” by looking at individual message send times.

Because of that schema reality, the old approach treated each chat row as a single “event” at one timestamp (typically created_at). That meant many sessions contained only one event, producing lots of 0-minute sessions and driving the median session duration to 0 even when users were actively chatting.

Example (old approach): what user activity looks like vs what we could measure

What the user actually does (message-level activity we don’t have as rows):
	•	09:00:10 user sends msg
	•	09:01:05 user sends msg
	•	09:03:30 user sends msg
	•	09:09:50 user sends msg
	•	09:12:00 user sends msg

What the old metric pipeline effectively saw (chat-row “event”):
	•	09:00:00 chat thread created (created_at)

Old computed session duration:
	•	09:00:00 – 09:00:00 = 0 minutes (even though the user was active until ~09:12)

We updated the logic to treat each chat thread as an activity interval using created_at → updated_at, then merge intervals into sessions with 5/10/20-minute gap thresholds. This produces a more realistic estimate of engagement time by measuring the span from first activity to last update, which is the best reliable signal available given our current schema.

Example (new approach): what we measure now

What we see in the chat table (activity interval per thread):
	•	09:00:00 chat thread created (created_at)
	•	09:12:00 chat thread last modified (updated_at)

New computed session duration (single thread):
	•	09:00:00 – 09:12:00 = 12 minutes

If the user touches multiple threads close together, we merge them into one session:
	•	09:00:00–09:06:00 Chat A active interval
	•	09:07:00–09:12:00 Chat B active interval
	•	Gap between intervals = 1 minute → merged into one continuous session
	•	New session duration = 09:00:00 – 09:12:00 = 12 minutes



Explaination part 2:
First: what changed (plain-English, with an example)

In your OpenWebUI database, the chat table is one row per chat thread, not one row per message. You have:
	•	created_at (when the chat thread was created)
	•	updated_at (when that thread was last modified—usually when the last message was added)
	•	chat (JSON blob that may contain messages, but you don’t have a separate populated messages table with message timestamps)

Because your messages table is empty, the old approach could not see individual message sends as separate timestamped rows. So the old code ended up treating each chat thread row as a single “event.”

Old approach (what it effectively measured)
It treated each chat row as a point-in-time event at created_at, then formed sessions from those points.

What user actually did (message-level activity we don’t have as rows):
	•	09:00:10 user sends msg
	•	09:01:05 user sends msg
	•	09:03:30 user sends msg
	•	09:09:50 user sends msg
	•	09:12:00 user sends msg

What the old code actually “saw” (chat-row event):
	•	09:00:00 chat thread created (created_at)

Old computed session duration:
	•	09:00:00 – 09:00:00 = 0 minutes

That’s why your median session duration was often 0: many “sessions” had only one observed event.

New approach (what it measures now)
We treat each chat thread as an activity interval using created_at → updated_at, then merge intervals into sessions using the 5/10/20-minute gap thresholds.

What we can measure reliably from the chat table:
	•	09:00:00 chat created (created_at)
	•	09:12:00 chat last modified (updated_at)

New computed duration for that interval/session:
	•	09:00:00 – 09:12:00 = 12 minutes

So: we still can’t measure per-message interaction time perfectly without message-level timestamps, but using updated_at gives a much more realistic proxy than using created_at alone.

⸻

1) Updated manager synopsis (short, includes examples)

Update to OpenWebUI usage session metrics

Our original session-duration calculation used the chat table as an “event stream” because the messages table is empty in our deployment. The chat table is one row per chat thread, so using only created_at effectively gave us one timestamp per thread (not per message). This caused many sessions to appear as single “events,” producing many 0-minute sessions and driving the median session duration to 0 even when users were actively chatting.

Old example (what user does vs what we could measure):
	•	09:00:10 user sends msg
	•	09:01:05 user sends msg
	•	09:03:30 user sends msg
	•	09:09:50 user sends msg
	•	09:12:00 user sends msg
Old pipeline mostly saw:
	•	09:00:00 chat created (created_at) → session duration computed as 0 minutes

We updated the logic to treat each chat thread as an activity interval using created_at → updated_at, then merge intervals into sessions using 5/10/20-minute gap thresholds.

New example:
	•	09:00:00 chat created (created_at)
	•	09:12:00 chat last modified (updated_at)
New pipeline measures the activity span as ~12 minutes, which is a more realistic proxy given the current schema.

⸻

2) Potential impacts of switching to updated_at
	1.	Better than 0-minute sessions, but still a proxy
This measures “thread activity span” (first create → last update), not necessarily continuous attention.
	2.	Can overestimate engagement for users who revisit a thread later
If a chat thread is created and then updated long after (e.g., same thread reused days later), the interval can look “long” even though the user wasn’t continuously active.
	3.	Means become very sensitive to long-tail outliers
A small number of long-lived threads can inflate mean session minutes dramatically. Median is usually more representative.
	4.	Gap thresholds (5/10/20) should mainly affect session counts/durations, not user counts
With your latest fixes, unique_users_total and weekly_active_users_total should stay consistent across gaps (good).
	5.	Best possible accuracy still requires message-level timestamps
If later you populate messages or can reliably parse per-message timestamps from the chat JSON, you can move from proxy → true interaction measurement.

⸻

3) Insights from your output (what the numbers mean)

From the screenshot (key columns):

Adoption / user counts
	•	Last 30 days: unique_users_total = 13,306
	•	October 2025: unique_users_total = 9,758

So the last-30-days window had ~36% more unique users than October.

“Consistent” users (weekly active)
	•	Last 30 days: weekly_active_users_total = 2,498
→ ~18.8% of uniques (2,498 / 13,306)
	•	October 2025: weekly_active_users_total = 2,426
→ ~24.9% of uniques (2,426 / 9,758)

Interpretation: October had fewer total users, but a higher share of users were “consistent weekly actives” under your Mon–Thu rule.

Active user-weeks (consistency volume)
	•	Last 30 days: active_user_weeks_total = 3,795
	•	October 2025: active_user_weeks_total = 3,960

Even with fewer unique users in October, you got slightly more active weeks, meaning your weekly-actives in October tended to show up as “active” in more weeks:
	•	Last 30 days: ~1.52 active weeks per weekly-active user
	•	October: ~1.63 active weeks per weekly-active user

Sessions per active week (usage frequency)

Mean sessions per active week drops as gap increases (expected—larger gap merges more):
	•	Last 30 days: mean sessions/active week ≈ 4.83 → 4.16 → 3.88 (5m → 10m → 20m)
	•	October: mean sessions/active week ≈ 4.06 → 3.21 → 3.03

Medians:
	•	Last 30 days median sessions/active week ≈ 3
	•	October median sessions/active week ≈ 2

Interpretation: among consistent users, typical weekly behavior looks like 2–3 sessions per week, with the mean higher due to heavier users.

Session duration: median vs mean (big warning flag)

You’re seeing:
	•	Median session minutes: small (roughly ~1.8 to ~6.9 minutes, increasing with gap as sessions merge)
	•	Mean session minutes: extremely large (hundreds to >1000 minutes)

That combination means the distribution is highly skewed: most sessions are short, but a minority are extremely long (and dominate the mean). This is consistent with the created_at → updated_at interval capturing thread lifespan (or sporadic reuse) rather than continuous attention.

How to interpret this responsibly:
	•	Treat median session minutes as “typical session length proxy.”
	•	Treat mean session minutes as “thread span is sometimes very long; long-tail exists” (not as typical engagement).
	•	If you want a more stable “average,” consider adding:
	•	p90 / p95 session duration
	•	winsorized/capped mean (e.g., cap session duration at 60 or 120 minutes for the average)

