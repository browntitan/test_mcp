Our original session-duration calculation used the chat table as an â€œevent streamâ€ because the messages table is empty in our deployment. In our database, the chat table is essentially one row per chat thread, with created_at and updated_at stored as epoch timestamps (BIGINT), and a chat JSON blob that contains the conversation content/metadata. We donâ€™t currently have a populated, normalized message table that stores one row per message with its own timestamp, so we canâ€™t reliably compute â€œtime in sessionâ€ by looking at individual message send times.

Because of that schema reality, the old approach treated each chat row as a single â€œeventâ€ at one timestamp (typically created_at). That meant many sessions contained only one event, producing lots of 0-minute sessions and driving the median session duration to 0 even when users were actively chatting.

Example (old approach): what user activity looks like vs what we could measure

What the user actually does (message-level activity we donâ€™t have as rows):
	â€¢	09:00:10 user sends msg
	â€¢	09:01:05 user sends msg
	â€¢	09:03:30 user sends msg
	â€¢	09:09:50 user sends msg
	â€¢	09:12:00 user sends msg

What the old metric pipeline effectively saw (chat-row â€œeventâ€):
	â€¢	09:00:00 chat thread created (created_at)

Old computed session duration:
	â€¢	09:00:00 â€“ 09:00:00 = 0 minutes (even though the user was active until ~09:12)

We updated the logic to treat each chat thread as an activity interval using created_at â†’ updated_at, then merge intervals into sessions with 5/10/20-minute gap thresholds. This produces a more realistic estimate of engagement time by measuring the span from first activity to last update, which is the best reliable signal available given our current schema.

Example (new approach): what we measure now

What we see in the chat table (activity interval per thread):
	â€¢	09:00:00 chat thread created (created_at)
	â€¢	09:12:00 chat thread last modified (updated_at)

New computed session duration (single thread):
	â€¢	09:00:00 â€“ 09:12:00 = 12 minutes

If the user touches multiple threads close together, we merge them into one session:
	â€¢	09:00:00â€“09:06:00 Chat A active interval
	â€¢	09:07:00â€“09:12:00 Chat B active interval
	â€¢	Gap between intervals = 1 minute â†’ merged into one continuous session
	â€¢	New session duration = 09:00:00 â€“ 09:12:00 = 12 minutes



Explaination part 2:
First: what changed (plain-English, with an example)

In your OpenWebUI database, the chat table is one row per chat thread, not one row per message. You have:
	â€¢	created_at (when the chat thread was created)
	â€¢	updated_at (when that thread was last modifiedâ€”usually when the last message was added)
	â€¢	chat (JSON blob that may contain messages, but you donâ€™t have a separate populated messages table with message timestamps)

Because your messages table is empty, the old approach could not see individual message sends as separate timestamped rows. So the old code ended up treating each chat thread row as a single â€œevent.â€

Old approach (what it effectively measured)
It treated each chat row as a point-in-time event at created_at, then formed sessions from those points.

What user actually did (message-level activity we donâ€™t have as rows):
	â€¢	09:00:10 user sends msg
	â€¢	09:01:05 user sends msg
	â€¢	09:03:30 user sends msg
	â€¢	09:09:50 user sends msg
	â€¢	09:12:00 user sends msg

What the old code actually â€œsawâ€ (chat-row event):
	â€¢	09:00:00 chat thread created (created_at)

Old computed session duration:
	â€¢	09:00:00 â€“ 09:00:00 = 0 minutes

Thatâ€™s why your median session duration was often 0: many â€œsessionsâ€ had only one observed event.

New approach (what it measures now)
We treat each chat thread as an activity interval using created_at â†’ updated_at, then merge intervals into sessions using the 5/10/20-minute gap thresholds.

What we can measure reliably from the chat table:
	â€¢	09:00:00 chat created (created_at)
	â€¢	09:12:00 chat last modified (updated_at)

New computed duration for that interval/session:
	â€¢	09:00:00 â€“ 09:12:00 = 12 minutes

So: we still canâ€™t measure per-message interaction time perfectly without message-level timestamps, but using updated_at gives a much more realistic proxy than using created_at alone.

â¸»

1) Updated manager synopsis (short, includes examples)

Update to OpenWebUI usage session metrics

Our original session-duration calculation used the chat table as an â€œevent streamâ€ because the messages table is empty in our deployment. The chat table is one row per chat thread, so using only created_at effectively gave us one timestamp per thread (not per message). This caused many sessions to appear as single â€œevents,â€ producing many 0-minute sessions and driving the median session duration to 0 even when users were actively chatting.

Old example (what user does vs what we could measure):
	â€¢	09:00:10 user sends msg
	â€¢	09:01:05 user sends msg
	â€¢	09:03:30 user sends msg
	â€¢	09:09:50 user sends msg
	â€¢	09:12:00 user sends msg
Old pipeline mostly saw:
	â€¢	09:00:00 chat created (created_at) â†’ session duration computed as 0 minutes

We updated the logic to treat each chat thread as an activity interval using created_at â†’ updated_at, then merge intervals into sessions using 5/10/20-minute gap thresholds.

New example:
	â€¢	09:00:00 chat created (created_at)
	â€¢	09:12:00 chat last modified (updated_at)
New pipeline measures the activity span as ~12 minutes, which is a more realistic proxy given the current schema.

â¸»

2) Potential impacts of switching to updated_at
	1.	Better than 0-minute sessions, but still a proxy
This measures â€œthread activity spanâ€ (first create â†’ last update), not necessarily continuous attention.
	2.	Can overestimate engagement for users who revisit a thread later
If a chat thread is created and then updated long after (e.g., same thread reused days later), the interval can look â€œlongâ€ even though the user wasnâ€™t continuously active.
	3.	Means become very sensitive to long-tail outliers
A small number of long-lived threads can inflate mean session minutes dramatically. Median is usually more representative.
	4.	Gap thresholds (5/10/20) should mainly affect session counts/durations, not user counts
With your latest fixes, unique_users_total and weekly_active_users_total should stay consistent across gaps (good).
	5.	Best possible accuracy still requires message-level timestamps
If later you populate messages or can reliably parse per-message timestamps from the chat JSON, you can move from proxy â†’ true interaction measurement.

â¸»

3) Insights from your output (what the numbers mean)

From the screenshot (key columns):

Adoption / user counts
	â€¢	Last 30 days: unique_users_total = 13,306
	â€¢	October 2025: unique_users_total = 9,758

So the last-30-days window had ~36% more unique users than October.

â€œConsistentâ€ users (weekly active)
	â€¢	Last 30 days: weekly_active_users_total = 2,498
â†’ ~18.8% of uniques (2,498 / 13,306)
	â€¢	October 2025: weekly_active_users_total = 2,426
â†’ ~24.9% of uniques (2,426 / 9,758)

Interpretation: October had fewer total users, but a higher share of users were â€œconsistent weekly activesâ€ under your Monâ€“Thu rule.

Active user-weeks (consistency volume)
	â€¢	Last 30 days: active_user_weeks_total = 3,795
	â€¢	October 2025: active_user_weeks_total = 3,960

Even with fewer unique users in October, you got slightly more active weeks, meaning your weekly-actives in October tended to show up as â€œactiveâ€ in more weeks:
	â€¢	Last 30 days: ~1.52 active weeks per weekly-active user
	â€¢	October: ~1.63 active weeks per weekly-active user

Sessions per active week (usage frequency)

Mean sessions per active week drops as gap increases (expectedâ€”larger gap merges more):
	â€¢	Last 30 days: mean sessions/active week â‰ˆ 4.83 â†’ 4.16 â†’ 3.88 (5m â†’ 10m â†’ 20m)
	â€¢	October: mean sessions/active week â‰ˆ 4.06 â†’ 3.21 â†’ 3.03

Medians:
	â€¢	Last 30 days median sessions/active week â‰ˆ 3
	â€¢	October median sessions/active week â‰ˆ 2

Interpretation: among consistent users, typical weekly behavior looks like 2â€“3 sessions per week, with the mean higher due to heavier users.

Session duration: median vs mean (big warning flag)

Youâ€™re seeing:
	â€¢	Median session minutes: small (roughly ~1.8 to ~6.9 minutes, increasing with gap as sessions merge)
	â€¢	Mean session minutes: extremely large (hundreds to >1000 minutes)

That combination means the distribution is highly skewed: most sessions are short, but a minority are extremely long (and dominate the mean). This is consistent with the created_at â†’ updated_at interval capturing thread lifespan (or sporadic reuse) rather than continuous attention.

How to interpret this responsibly:
	â€¢	Treat median session minutes as â€œtypical session length proxy.â€
	â€¢	Treat mean session minutes as â€œthread span is sometimes very long; long-tail existsâ€ (not as typical engagement).
	â€¢	If you want a more stable â€œaverage,â€ consider adding:
	â€¢	p90 / p95 session duration
	â€¢	winsorized/capped mean (e.g., cap session duration at 60 or 120 minutes for the average)




_______________________________
Below is a manager-friendly set of answers to each question. Iâ€™ll put my responses in ğŸŸ¦ BLUE (using the blue square) so you can paste this directly into your doc/email.

â¸»

1) â€œBy session do we essentially mean thread? (Iâ€™m assuming merges across threads are minimal?)â€

ğŸŸ¦ Answer: A session is not the same as a chat thread in our code.
	â€¢	A chat thread = one row in the chat table. We treat each thread as an activity interval: [created_at â†’ updated_at].
	â€¢	A session = a continuous usage window for a single user, formed by merging one or more thread-intervals when theyâ€™re close in time (within the gap: 5/10/20 minutes).

ğŸŸ¦ What that means practically:
	â€¢	If a user only interacts with one thread and then stops, that often becomes one session â‰ˆ one thread.
	â€¢	If a user bounces between threads or updates multiple threads close together, those thread-intervals can merge into one session (especially at 10m or 20m gap).
So the â€œsession â‰ˆ threadâ€ assumption is sometimes true, but not guaranteed.

ğŸŸ¦ How we can tell whether merging is minimal or significant:
The fact that median session minutes increases a lot when gap increases is a signal that merging/bridging is happening (either within a thread over short pauses, or across threads close together). If merges were truly rare, changing gap wouldnâ€™t move median duration much.

1a) â€œMedian session minutes triples while the median number of sessions stays constantâ€”what does that imply?â€

ğŸŸ¦ Answer: That pattern is exactly what weâ€™d expect when a larger gap merges short breaks into the same session, making each session longer, without collapsing separate â€œwork burstsâ€ into fewer sessions.
	â€¢	Median sessions/week stays constant â†’ users still have roughly the same number of distinct work bursts per week.
	â€¢	Median session minutes increases â†’ within each burst, the user may pause briefly (or switch threads), and the larger gap treats that as the same session.

ğŸŸ¦ Interpretation using your numbers (Last 30 days):
	â€¢	Median sessions per active week stays at 3 (5m/10m/20m)
	â€¢	Median session minutes increases from ~1.78 â†’ ~4.43 â†’ ~6.17
So: users are still doing ~3 bursts/week, but weâ€™re grouping those bursts more aggressively as we increase the gap.

â¸»

2) â€œThe mean and median session minutes are crazy different.â€

ğŸŸ¦ Answer: Yesâ€”this is a strong sign of a highly skewed distribution (a long tail of very large session durations). The median reflects â€œtypical behavior,â€ while the mean is being pulled up by a smaller number of extremely long sessions.

ğŸŸ¦ Why the long tail happens in our specific approach:
We define a thread interval as created_at â†’ updated_at. If a thread is created and then updated much later, it creates a very long intervalâ€”even if the user wasnâ€™t continuously engaged that whole time. When those long intervals are included in sessions, they can create sessions measured in hours/days, which inflates the mean dramatically.

2a) â€œIs this due to folks who only use one chat over and over?â€

ğŸŸ¦ Partly, yes, but itâ€™s broader than that. â€œReusing the same threadâ€ can definitely create large updated_at - created_at spans. But even without thread reuse, any workflow where a thread is created and then updated much later can produce long spans.
So: thread reuse is one plausible contributor, but the bigger truth is: updated_at - created_at measures â€œthread lifespan,â€ not guaranteed continuous attention. Thatâ€™s why the mean can look huge.

ğŸŸ¦ Practical guidance: Treat median session minutes as the â€œtypical session length proxy.â€ Treat the mean as â€œthere exist long-lived sessions/threads,â€ not as typical engagement.

â¸»

3) â€œAre session minutes aggregated across the entire time to reach mean/median?â€

ğŸŸ¦ Answer: Yes. The mean/median session minutes are computed across all sessions (for the selected population) in that time window.
Each session contributes one duration value, and then we take mean/median across the whole set.

ğŸŸ¦ Important nuance: In your current outputs, mean_session_minutes / median_session_minutes are computed for sessions in weekly-active weeks (the â€œcoreâ€ cohort), not for every user. Thatâ€™s why you also have:
	â€¢	total_sessions_total (all users)
	â€¢	active_sessions_total (only weekly-active weeks)

3a) â€œIf someone starts a chat 10/1 and updates 10/31â€”are they counted as active each week?â€

ğŸŸ¦ Answer: Under our current approximation, a long thread interval can inflate â€œactive days/weeksâ€ more than weâ€™d ideally wantâ€”because we record â€œdays touchedâ€ by the interval.
However, we also have a guardrail: we cap the day-walk at 10 days for any single interval to avoid runaway counting.

So in your example (10/1 â†’ 10/31):
	â€¢	We would not count activity for every day/week in October, because we cap how many days we infer from one interval.
	â€¢	But it could still count multiple weekdays (and possibly more than one week) even if actual activity only occurred on 10/1 and 10/31.

ğŸŸ¦ Interpretation: This is a limitation of using created_at â†’ updated_at as a proxy for â€œdays active.â€ Without message-level timestamps, we cannot perfectly know which days inside that span had real interaction.

3b) â€œAre session minutes only for active users/active weeks?â€

ğŸŸ¦ Answer: Yes, for the session-minute stats currently shown:
	â€¢	mean_session_minutes, median_session_minutes, mean_sessions_per_active_week, median_sessions_per_active_week, and active_sessions_total are tied to weekly-active weeks.
	â€¢	total_sessions_total is the counter you asked us to add that covers all sessions for all users, independent of weekly-active status.

â¸»

Quick interpretation of your specific outputs (what stands out)

Using the table you shared:

A) Adoption (unique users)
	â€¢	Last 30 Days: ~13,373 unique users
	â€¢	October 2025: ~9,757 unique users
ğŸŸ¦ Meaning: more people tried/used OpenWebUI in the last 30 days than in October (workdays only).

B) Core user base (weekly active users)
	â€¢	Last 30 Days: 2,422 weekly active users
	â€¢	October 2025: 2,426 weekly active users
ğŸŸ¦ Meaning: the core consistent user base is basically flat between the two periods, even though overall unique users changed a lot.

ğŸŸ¦ Key takeaway: growth in last 30 days seems to be coming more from occasional users, not growth in the core weekly-active population.

C) Total sessions vs active sessions

Last 30 Days:
	â€¢	total_sessions_total: 81,067 â†’ 76,154 â†’ 73,196 (5mâ†’10mâ†’20m)
	â€¢	active_sessions_total: 17,042 â†’ 14,607 â†’ 13,584

October:
	â€¢	total_sessions_total: 57,519 â†’ 52,824 â†’ 51,014
	â€¢	active_sessions_total: 16,072 â†’ 12,718 â†’ 11,987

ğŸŸ¦ Meaning: most sessions are generated by the broader population (total sessions), while a smaller fraction comes from weekly-active weeks (active sessions).
You can use active_sessions_total / total_sessions_total as â€œshare of sessions attributable to core cohort.â€

D) Active user-weeks (core consistency volume)
	â€¢	Last 30 Days: active_user_weeks_total = 3,575
	â€¢	October 2025: active_user_weeks_total = 3,960

ğŸŸ¦ Meaning: October had more active weeks among core users, even with fewer overall users. This suggests Octoberâ€™s weekly-actives were â€œactiveâ€ in more distinct weeks.

â¸»

If you want one â€œtightâ€ response to your manager:

ğŸŸ¦ I can also rewrite the above into a short paragraph-per-question format suitable for email/Slackâ€”tell me what tone you want (formal vs casual).